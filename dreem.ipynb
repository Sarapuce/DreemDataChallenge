{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SInXhoCEPaRe",
        "colab_type": "code",
        "outputId": "96685217-b433-4c54-adfa-0876be3c073a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiVznudbSenQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, cv2, h5py\n",
        "import torch\n",
        "import torchvision\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as PathEffects\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as utils\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from copy import deepcopy\n",
        "from time import time\n",
        "from mpl_toolkits import mplot3d\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "from torch import autograd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9BSSWEsUOa1",
        "colab_type": "code",
        "outputId": "4ac6d97e-0de1-4c02-8bf3-9c48efe434ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "X = h5py.File(\"./drive/My Drive/data/X_train.h5\", \"r\")[\"features\"][:]\n",
        "Y = pd.read_csv(\"./drive/My Drive/data/y_train.csv\").as_matrix()[:, 1].squeeze()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wDVZUl0Vp4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = len(X)\n",
        "x_train = X[:int(0.7*n), 11:]\n",
        "y_train = Y[:int(0.7*n)]\n",
        "x_test  = X[int(0.7*n):, 11:]\n",
        "y_test  = Y[int(0.7*n):]\n",
        "x_train = x_train[:, ::2]\n",
        "x_test  = x_test[:, ::2]\n",
        "x_train = np.array([[j for i, j in enumerate(eeg[10:]) if i % 6] for eeg in x_train])\n",
        "\n",
        "x_train_meta = X[:int(0.7*n), :11]\n",
        "x_test_meta = X[int(0.7*n):, :11]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNgP_2WAWjqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.clip(x_train, -205, 205)\n",
        "x_test  = np.clip(x_test, -205, 205)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhQ9K8fEbm14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train / 205\n",
        "x_test  = x_test / 205"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZgOlc6jQmll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxi = np.max(x_train_meta, axis = 0)\n",
        "mini = np.min(x_train_meta, axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gc178X8RSlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_meta = (x_train_meta - maxi) / (mini - maxi)\n",
        "x_test_meta  = (x_test_meta - maxi) / (mini - maxi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNrQ_50Np0FP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = len(X)\n",
        "x_train = X[:, 11:]\n",
        "y_train = Y[:]\n",
        "x_train = x_train[:, ::2]\n",
        "x_train = np.array([[j for i, j in enumerate(eeg[10:]) if i % 6] for eeg in x_train])\n",
        "x_train_meta = X[:, :11]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usR0OVBxqTpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.clip(x_train, -205, 205)\n",
        "x_train = x_train / 205\n",
        "maxi = np.max(x_train_meta, axis = 0)\n",
        "mini = np.min(x_train_meta, axis = 0)\n",
        "x_train_meta = (x_train_meta - maxi) / (mini - maxi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E56FCcjGex10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "def conv5x5(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv1d(in_planes, out_planes, kernel_size=5, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "def conv7x7(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv1d(in_planes, out_planes, kernel_size=7, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "\n",
        "class BasicBlock3x3(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes3, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock3x3, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes3, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm1d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm1d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class BasicBlock5x5(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes5, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock5x5, self).__init__()\n",
        "        self.conv1 = conv5x5(inplanes5, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm1d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv5x5(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm1d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        d = residual.shape[2] - out.shape[2]\n",
        "        out1 = residual[:,:,0:-d] + out\n",
        "        out1 = self.relu(out1)\n",
        "        # out += residual\n",
        "\n",
        "        return out1\n",
        "\n",
        "\n",
        "\n",
        "class BasicBlock7x7(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes7, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock7x7, self).__init__()\n",
        "        self.conv1 = conv7x7(inplanes7, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm1d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv7x7(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm1d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        d = residual.shape[2] - out.shape[2]\n",
        "        out1 = residual[:, :, 0:-d] + out\n",
        "        out1 = self.relu(out1)\n",
        "        # out += residual\n",
        "\n",
        "        return out1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MSResNet(nn.Module):\n",
        "    def __init__(self, input_channel, layers=[1, 1, 1, 1], num_classes=10):\n",
        "        self.inplanes3 = 64\n",
        "        self.inplanes5 = 64\n",
        "        self.inplanes7 = 64\n",
        "\n",
        "        super(MSResNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(input_channel, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer3x3_1 = self._make_layer3(BasicBlock3x3, 64, layers[0], stride=2)\n",
        "        self.layer3x3_2 = self._make_layer3(BasicBlock3x3, 128, layers[1], stride=2)\n",
        "        self.layer3x3_3 = self._make_layer3(BasicBlock3x3, 256, layers[2], stride=2)\n",
        "        # self.layer3x3_4 = self._make_layer3(BasicBlock3x3, 512, layers[3], stride=2)\n",
        "\n",
        "        # maxplooing kernel size: 16, 11, 6\n",
        "        self.maxpool3 = nn.AvgPool1d(kernel_size=16, stride=1, padding=0)\n",
        "\n",
        "\n",
        "        self.layer5x5_1 = self._make_layer5(BasicBlock5x5, 64, layers[0], stride=2)\n",
        "        self.layer5x5_2 = self._make_layer5(BasicBlock5x5, 128, layers[1], stride=2)\n",
        "        self.layer5x5_3 = self._make_layer5(BasicBlock5x5, 256, layers[2], stride=2)\n",
        "        # self.layer5x5_4 = self._make_layer5(BasicBlock5x5, 512, layers[3], stride=2)\n",
        "        self.maxpool5 = nn.AvgPool1d(kernel_size=11, stride=1, padding=0)\n",
        "\n",
        "\n",
        "        self.layer7x7_1 = self._make_layer7(BasicBlock7x7, 64, layers[0], stride=2)\n",
        "        self.layer7x7_2 = self._make_layer7(BasicBlock7x7, 128, layers[1], stride=2)\n",
        "        self.layer7x7_3 = self._make_layer7(BasicBlock7x7, 256, layers[2], stride=2)\n",
        "        # self.layer7x7_4 = self._make_layer7(BasicBlock7x7, 512, layers[3], stride=2)\n",
        "        self.maxpool7 = nn.AvgPool1d(kernel_size=6, stride=1, padding=0)\n",
        "\n",
        "        # self.drop = nn.Dropout(p=0.2)\n",
        "        self.fc = nn.Linear(256*3, 128)\n",
        "\n",
        "        # todo: modify the initialization\n",
        "        # for m in self.modules():\n",
        "        #     if isinstance(m, nn.Conv1d):\n",
        "        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "        #     elif isinstance(m, nn.BatchNorm1d):\n",
        "        #         m.weight.data.fill_(1)\n",
        "        #         m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer3(self, block, planes, blocks, stride=2):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes3 != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.inplanes3, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes3, planes, stride, downsample))\n",
        "        self.inplanes3 = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes3, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_layer5(self, block, planes, blocks, stride=2):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes5 != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.inplanes5, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes5, planes, stride, downsample))\n",
        "        self.inplanes5 = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes5, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def _make_layer7(self, block, planes, blocks, stride=2):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes7 != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.inplanes7, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes7, planes, stride, downsample))\n",
        "        self.inplanes7 = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes7, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x0):\n",
        "        x0 = self.conv1(x0)\n",
        "        x0 = self.bn1(x0)\n",
        "        x0 = self.relu(x0)\n",
        "        x0 = self.maxpool(x0)\n",
        "        x = self.layer3x3_1(x0)\n",
        "        x = self.layer3x3_2(x)\n",
        "        x = self.layer3x3_3(x)\n",
        "        # x = self.layer3x3_4(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        y = self.layer5x5_1(x0)\n",
        "        y = self.layer5x5_2(y)\n",
        "        y = self.layer5x5_3(y)\n",
        "        # y = self.layer5x5_4(y)\n",
        "        y = self.maxpool5(y)\n",
        "\n",
        "        z = self.layer7x7_1(x0)\n",
        "        z = self.layer7x7_2(z)\n",
        "        z = self.layer7x7_3(z)\n",
        "        # z = self.layer7x7_4(z)\n",
        "        z = self.maxpool7(z)\n",
        "\n",
        "        out = torch.cat([x, y, z], dim=1)\n",
        "        \n",
        "        out = out.squeeze()\n",
        "        # out = self.drop(out)\n",
        "        out1 = self.fc(out)\n",
        "\n",
        "        return out1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Cg0L7hoqRl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_distance(data):\n",
        "    dot_product = torch.matmul(data, data.transpose(1, 0))\n",
        "    diag        = torch.diag(dot_product)\n",
        "    distances   = diag.view((1, diag.shape[0])) - 2*dot_product + diag.view((diag.shape[0], 1))\n",
        "    distances   = torch.max(distances, torch.tensor([0]).float().cuda())\n",
        "    mask        = torch.eq(distances, torch.zeros(1).cuda()).float().cuda()\n",
        "    distances   = distances + mask * 1e-16\n",
        "    distances   = torch.sqrt(distances)\n",
        "    distances   = distances * (1.0 - mask)\n",
        "    return distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zf0eaC9qSh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mask_triplet(labels):\n",
        "    hot = F.one_hot(labels, 3).float()\n",
        "    positive_mask = torch.matmul(hot, hot.transpose(0, 1))\n",
        "    size = positive_mask.shape[0]\n",
        "    positive_mask[range(size), range(size)] = torch.zeros(positive_mask.shape[0]).long().cuda()\n",
        "    negative_mask = torch.matmul(torch.ones(hot.shape).long().cuda() - hot, hot.transpose(0, 1))\n",
        "    mask = torch.matmul(positive_mask.view((size, size, 1)), negative_mask.view((size, 1, size)))\n",
        "    return mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ALGO3A0qX6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def triplet_loss(data, labels, m):\n",
        "    distances = get_distance(data)\n",
        "    size = distances.shape[0]\n",
        "    delta_p   = distances.view((size, size, 1)).repeat((1, 1, size)) + m\n",
        "    delta_n   = distances.view((size, 1, size)).repeat((1, size, 1))\n",
        "    triplet_loss = delta_p - delta_n\n",
        "    mask_hard = torch.gt(delta_p, delta_n).float()\n",
        "    mask      = get_mask_triplet(labels).float() * mask_hard\n",
        "    triplet_loss = triplet_loss * mask\n",
        "    triplet_loss = torch.max(triplet_loss, torch.zeros(1).float().cuda())\n",
        "    \n",
        "    nb_of_tensor = torch.gt(triplet_loss, torch.tensor([1e-16]).float().cuda())\n",
        "    nb_of_tensor = torch.sum(nb_of_tensor)\n",
        "    triplet_loss = torch.sum(triplet_loss)\n",
        "    return triplet_loss / (nb_of_tensor + torch.tensor([1e-16]).float().cuda())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfhbxWgVrFXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augment(serie):\n",
        "    noise = torch.randn(serie.shape[0], 1, serie.shape[-1]).float().cuda() * 0.005\n",
        "    serie = noise + serie\n",
        "    coins = torch.gt(torch.randn((serie.shape[0], 1, 1)).float().cuda(), torch.tensor([0]).float().cuda()).float().cuda() * 2 - 1\n",
        "    serie = serie * coins\n",
        "    return serie"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKMWZmi9ri-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = torch.tensor(x_train).float().cuda()\n",
        "x_test  = torch.tensor(x_test).float().cuda()\n",
        "y_train = torch.tensor(y_train).long().cuda()\n",
        "y_test  = torch.tensor(y_test).long().cuda()\n",
        "x_train_meta = torch.tensor(x_train_meta).float().cuda()\n",
        "x_test_meta = torch.tensor(x_test_meta).float().cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KewAMnFCroWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.view((-1, 1, 512))\n",
        "x_test  = x_test.view((-1, 1, 512))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh55GzGjrr31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = utils.TensorDataset(x_train, x_train_meta, y_train)\n",
        "test_data  = utils.TensorDataset(x_test, x_test_meta, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ5pAM93sAVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = utils.DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader  = utils.DataLoader(test_data, batch_size=32, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g9sAlLIsMBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = torch.tensor(x_train).float().cuda()\n",
        "y_train = torch.tensor(y_train).long().cuda()\n",
        "x_train_meta = torch.tensor(x_train_meta).float().cuda()\n",
        "x_train = x_train.view((-1, 1, 512))\n",
        "train_data = utils.TensorDataset(x_train, x_train_meta, y_train)\n",
        "train_loader = utils.DataLoader(train_data, batch_size=32, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SWwoXlEbpQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MSResNet(1).cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbnpjwPtfVfF",
        "colab_type": "code",
        "outputId": "1ea18a8d-788c-42fc-c234-e690e2517d78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 10\n",
        "best_loss = 1000\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    test_loss    = 0\n",
        "    for i, (x, _, y) in enumerate(train_loader):\n",
        "        x = augment(x)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss   = triplet_loss(output, y, torch.tensor([1.]).float().cuda())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if not i % 500:\n",
        "          print('Current loss : {0:.2f}'.format(loss.item()))\n",
        "   \n",
        "torch.save(model, '/content/drive/My Drive/data/model_emb.pt')\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current loss : 1.59\n",
            "Current loss : 0.96\n",
            "Current loss : 1.05\n",
            "Current loss : 0.96\n",
            "Current loss : 1.05\n",
            "Current loss : 0.95\n",
            "Current loss : 1.06\n",
            "Current loss : 1.04\n",
            "Current loss : 0.99\n",
            "Current loss : 0.85\n",
            "Current loss : 1.05\n",
            "Current loss : 0.98\n",
            "Current loss : 0.89\n",
            "Current loss : 1.00\n",
            "Current loss : 0.98\n",
            "Current loss : 0.88\n",
            "Current loss : 1.00\n",
            "Current loss : 1.04\n",
            "Current loss : 1.07\n",
            "Current loss : 1.01\n",
            "Current loss : 0.93\n",
            "Current loss : 1.03\n",
            "Current loss : 1.04\n",
            "Current loss : 1.03\n",
            "Current loss : 1.02\n",
            "Current loss : 0.98\n",
            "Current loss : 1.08\n",
            "Current loss : 1.09\n",
            "Current loss : 0.96\n",
            "Current loss : 1.02\n",
            "Current loss : 1.01\n",
            "Current loss : 1.04\n",
            "Current loss : 1.05\n",
            "Current loss : 0.98\n",
            "Current loss : 0.80\n",
            "Current loss : 1.06\n",
            "Current loss : 1.03\n",
            "Current loss : 0.97\n",
            "Current loss : 1.03\n",
            "Current loss : 0.99\n",
            "Current loss : 1.13\n",
            "Current loss : 1.10\n",
            "Current loss : 0.99\n",
            "Current loss : 0.99\n",
            "Current loss : 0.93\n",
            "Current loss : 1.09\n",
            "Current loss : 0.97\n",
            "Current loss : 1.00\n",
            "Current loss : 0.86\n",
            "Current loss : 1.08\n",
            "Current loss : 0.97\n",
            "Current loss : 0.88\n",
            "Current loss : 0.95\n",
            "Current loss : 0.98\n",
            "Current loss : 1.08\n",
            "Current loss : 1.07\n",
            "Current loss : 1.02\n",
            "Current loss : 1.07\n",
            "Current loss : 1.07\n",
            "Current loss : 0.99\n",
            "Current loss : 0.99\n",
            "Current loss : 0.94\n",
            "Current loss : 0.88\n",
            "Current loss : 0.98\n",
            "Current loss : 0.94\n",
            "Current loss : 1.13\n",
            "Current loss : 0.88\n",
            "Current loss : 1.04\n",
            "Current loss : 1.01\n",
            "Current loss : 1.01\n",
            "Current loss : 1.09\n",
            "Current loss : 1.02\n",
            "Current loss : 1.04\n",
            "Current loss : 1.09\n",
            "Current loss : 0.93\n",
            "Current loss : 1.05\n",
            "Current loss : 1.13\n",
            "Current loss : 1.06\n",
            "Current loss : 0.88\n",
            "Current loss : 1.12\n",
            "Current loss : 1.07\n",
            "Current loss : 1.04\n",
            "Current loss : 0.96\n",
            "Current loss : 0.97\n",
            "Current loss : 1.01\n",
            "Current loss : 0.92\n",
            "Current loss : 1.09\n",
            "Current loss : 1.00\n",
            "Current loss : 1.04\n",
            "Current loss : 0.91\n",
            "Current loss : 1.03\n",
            "Current loss : 1.04\n",
            "Current loss : 1.00\n",
            "Current loss : 1.09\n",
            "Current loss : 1.00\n",
            "Current loss : 0.92\n",
            "Current loss : 1.00\n",
            "Current loss : 1.06\n",
            "Current loss : 0.95\n",
            "Current loss : 0.92\n",
            "Current loss : 0.93\n",
            "Current loss : 1.06\n",
            "Current loss : 1.02\n",
            "Current loss : 1.02\n",
            "Current loss : 1.03\n",
            "Current loss : 0.95\n",
            "Current loss : 0.99\n",
            "Current loss : 1.05\n",
            "Current loss : 0.95\n",
            "Current loss : 0.98\n",
            "Current loss : 0.97\n",
            "Current loss : 1.00\n",
            "Current loss : 1.02\n",
            "Current loss : 0.98\n",
            "Current loss : 1.01\n",
            "Current loss : 0.98\n",
            "Current loss : 0.99\n",
            "Current loss : 1.10\n",
            "Current loss : 1.10\n",
            "Current loss : 1.01\n",
            "Current loss : 1.02\n",
            "Current loss : 0.99\n",
            "Current loss : 0.95\n",
            "Current loss : 0.95\n",
            "Current loss : 1.05\n",
            "Current loss : 1.04\n",
            "Current loss : 1.01\n",
            "Current loss : 1.01\n",
            "Current loss : 1.01\n",
            "Current loss : 1.06\n",
            "Current loss : 0.99\n",
            "Current loss : 1.15\n",
            "Current loss : 1.04\n",
            "Current loss : 1.12\n",
            "Current loss : 1.11\n",
            "Current loss : 1.08\n",
            "Current loss : 0.98\n",
            "Current loss : 0.88\n",
            "Current loss : 1.02\n",
            "Current loss : 0.91\n",
            "Current loss : 0.84\n",
            "Current loss : 1.03\n",
            "Current loss : 0.95\n",
            "Current loss : 0.97\n",
            "Current loss : 0.99\n",
            "Current loss : 1.03\n",
            "Current loss : 1.01\n",
            "Current loss : 0.99\n",
            "Current loss : 1.07\n",
            "Current loss : 1.09\n",
            "Current loss : 1.07\n",
            "Current loss : 0.87\n",
            "Current loss : 0.98\n",
            "Current loss : 0.88\n",
            "Current loss : 1.08\n",
            "Current loss : 1.06\n",
            "Current loss : 1.04\n",
            "Current loss : 1.05\n",
            "Current loss : 0.93\n",
            "Current loss : 0.96\n",
            "Current loss : 1.03\n",
            "Current loss : 1.01\n",
            "Current loss : 0.96\n",
            "Current loss : 1.01\n",
            "Current loss : 1.04\n",
            "Current loss : 0.92\n",
            "Current loss : 0.98\n",
            "Current loss : 1.02\n",
            "Current loss : 1.02\n",
            "Current loss : 1.02\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MSResNet. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv1d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm1d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool1d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicBlock3x3. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type AvgPool1d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicBlock5x5. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicBlock7x7. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-Rye0kPTMz6",
        "colab_type": "code",
        "outputId": "2c653c3d-0e0f-4564-bb85-04dbe3eef1cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = torch.load('/content/drive/My Drive/data/model_emb.pt')\n",
        "model.eval()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MSResNet(\n",
              "  (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)\n",
              "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer3x3_1): Sequential(\n",
              "    (0): BasicBlock3x3(\n",
              "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
              "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv1d(64, 64, kernel_size=(1,), stride=(2,), bias=False)\n",
              "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer3x3_2): Sequential(\n",
              "    (0): BasicBlock3x3(\n",
              "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
              "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
              "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer3x3_3): Sequential(\n",
              "    (0): BasicBlock3x3(\n",
              "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
              "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
              "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (maxpool3): AvgPool1d(kernel_size=(16,), stride=(1,), padding=(0,))\n",
              "  (layer5x5_1): Sequential(\n",
              "    (0): BasicBlock5x5(\n",
              "      (conv1): Conv1d(64, 64, kernel_size=(5,), stride=(2,), padding=(1,), bias=False)\n",
              "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(1,), bias=False)\n",
              "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv1d(64, 64, kernel_size=(1,), stride=(2,), bias=False)\n",
              "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer5x5_2): Sequential(\n",
              "    (0): BasicBlock5x5(\n",
              "      (conv1): Conv1d(64, 128, kernel_size=(5,), stride=(2,), padding=(1,), bias=False)\n",
              "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(1,), bias=False)\n",
              "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
              "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer5x5_3): Sequential(\n",
              "    (0): BasicBlock5x5(\n",
              "      (conv1): Conv1d(128, 256, kernel_size=(5,), stride=(2,), padding=(1,), bias=False)\n",
              "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(1,), bias=False)\n",
              "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
              "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (maxpool5): AvgPool1d(kernel_size=(11,), stride=(1,), padding=(0,))\n",
              "  (layer7x7_1): Sequential(\n",
              "    (0): BasicBlock7x7(\n",
              "      (conv1): Conv1d(64, 64, kernel_size=(7,), stride=(2,), padding=(1,), bias=False)\n",
              "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(1,), bias=False)\n",
              "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv1d(64, 64, kernel_size=(1,), stride=(2,), bias=False)\n",
              "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer7x7_2): Sequential(\n",
              "    (0): BasicBlock7x7(\n",
              "      (conv1): Conv1d(64, 128, kernel_size=(7,), stride=(2,), padding=(1,), bias=False)\n",
              "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(1,), bias=False)\n",
              "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
              "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer7x7_3): Sequential(\n",
              "    (0): BasicBlock7x7(\n",
              "      (conv1): Conv1d(128, 256, kernel_size=(7,), stride=(2,), padding=(1,), bias=False)\n",
              "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(1,), bias=False)\n",
              "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
              "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (maxpool7): AvgPool1d(kernel_size=(6,), stride=(1,), padding=(0,))\n",
              "  (fc): Linear(in_features=768, out_features=128, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdt8-YWVfYRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1   = nn.Linear(128+11, 128)\n",
        "        self.fc2   = nn.Linear(128, 32)\n",
        "        self.fc3   = nn.Linear(32, 3)\n",
        "        self.drop1 = nn.Dropout(0.3)\n",
        "        self.drop2 = nn.Dropout(0.2)\n",
        "        \n",
        "  def forward(self, x, eeg):\n",
        "    x = torch.cat([x, eeg], -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.drop1(x)\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.drop2(x)\n",
        "    x = self.fc3(x)\n",
        "    return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrbUP7QEfdgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Net().cuda()\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buGtfVZulFYZ",
        "colab_type": "code",
        "outputId": "83c0aa5f-1dc5-4627-c81b-6ff6de8cd8f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 9\n",
        "best_loss = 1000000\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    test_loss    = 0\n",
        "    for i, (x, meta, y) in enumerate(train_loader):\n",
        "        x = augment(x)\n",
        "        optimizer.zero_grad()\n",
        "        embedding = model(x)\n",
        "        \n",
        "        output = net(meta, embedding)\n",
        "        loss = loss_function(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if not i % 500:\n",
        "          print('Current loss : {0:.2f}'.format(loss.item()))\n",
        "    if running_loss < best_loss:\n",
        "      torch.save(model, '/content/drive/My Drive/data/netyouhou.pt')\n",
        "      best_loss = running_loss"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current loss : 0.98\n",
            "Current loss : 0.89\n",
            "Current loss : 0.99\n",
            "Current loss : 1.07\n",
            "Current loss : 0.89\n",
            "Current loss : 0.96\n",
            "Current loss : 0.99\n",
            "Current loss : 1.08\n",
            "Current loss : 0.99\n",
            "Current loss : 0.99\n",
            "Current loss : 0.85\n",
            "Current loss : 0.83\n",
            "Current loss : 0.98\n",
            "Current loss : 0.89\n",
            "Current loss : 0.99\n",
            "Current loss : 0.99\n",
            "Current loss : 1.06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MSResNet. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv1d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm1d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool1d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicBlock3x3. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type AvgPool1d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicBlock5x5. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicBlock7x7. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Current loss : 1.19\n",
            "Current loss : 0.96\n",
            "Current loss : 1.05\n",
            "Current loss : 0.95\n",
            "Current loss : 1.05\n",
            "Current loss : 0.84\n",
            "Current loss : 0.99\n",
            "Current loss : 1.08\n",
            "Current loss : 0.97\n",
            "Current loss : 0.89\n",
            "Current loss : 0.92\n",
            "Current loss : 0.87\n",
            "Current loss : 0.99\n",
            "Current loss : 0.98\n",
            "Current loss : 0.92\n",
            "Current loss : 0.85\n",
            "Current loss : 1.09\n",
            "Current loss : 0.92\n",
            "Current loss : 0.89\n",
            "Current loss : 1.09\n",
            "Current loss : 0.92\n",
            "Current loss : 1.02\n",
            "Current loss : 0.93\n",
            "Current loss : 0.98\n",
            "Current loss : 1.00\n",
            "Current loss : 1.04\n",
            "Current loss : 0.90\n",
            "Current loss : 1.04\n",
            "Current loss : 0.88\n",
            "Current loss : 0.86\n",
            "Current loss : 0.93\n",
            "Current loss : 1.10\n",
            "Current loss : 0.97\n",
            "Current loss : 0.96\n",
            "Current loss : 1.00\n",
            "Current loss : 0.95\n",
            "Current loss : 0.93\n",
            "Current loss : 0.98\n",
            "Current loss : 1.10\n",
            "Current loss : 1.02\n",
            "Current loss : 1.00\n",
            "Current loss : 1.04\n",
            "Current loss : 1.17\n",
            "Current loss : 0.93\n",
            "Current loss : 1.07\n",
            "Current loss : 0.97\n",
            "Current loss : 0.98\n",
            "Current loss : 0.90\n",
            "Current loss : 0.98\n",
            "Current loss : 0.86\n",
            "Current loss : 0.98\n",
            "Current loss : 0.88\n",
            "Current loss : 0.99\n",
            "Current loss : 1.06\n",
            "Current loss : 0.88\n",
            "Current loss : 1.08\n",
            "Current loss : 0.91\n",
            "Current loss : 1.16\n",
            "Current loss : 0.92\n",
            "Current loss : 0.83\n",
            "Current loss : 0.87\n",
            "Current loss : 1.10\n",
            "Current loss : 0.99\n",
            "Current loss : 0.87\n",
            "Current loss : 1.00\n",
            "Current loss : 0.96\n",
            "Current loss : 0.88\n",
            "Current loss : 0.85\n",
            "Current loss : 0.95\n",
            "Current loss : 0.85\n",
            "Current loss : 1.02\n",
            "Current loss : 0.96\n",
            "Current loss : 1.03\n",
            "Current loss : 0.99\n",
            "Current loss : 0.93\n",
            "Current loss : 0.94\n",
            "Current loss : 0.92\n",
            "Current loss : 0.91\n",
            "Current loss : 0.94\n",
            "Current loss : 0.94\n",
            "Current loss : 1.00\n",
            "Current loss : 1.00\n",
            "Current loss : 0.67\n",
            "Current loss : 1.01\n",
            "Current loss : 0.95\n",
            "Current loss : 0.94\n",
            "Current loss : 1.03\n",
            "Current loss : 0.89\n",
            "Current loss : 1.09\n",
            "Current loss : 0.85\n",
            "Current loss : 1.12\n",
            "Current loss : 1.05\n",
            "Current loss : 0.97\n",
            "Current loss : 0.96\n",
            "Current loss : 0.84\n",
            "Current loss : 0.84\n",
            "Current loss : 0.87\n",
            "Current loss : 1.26\n",
            "Current loss : 1.10\n",
            "Current loss : 1.00\n",
            "Current loss : 0.97\n",
            "Current loss : 0.97\n",
            "Current loss : 1.06\n",
            "Current loss : 0.88\n",
            "Current loss : 0.99\n",
            "Current loss : 0.87\n",
            "Current loss : 0.95\n",
            "Current loss : 1.06\n",
            "Current loss : 1.01\n",
            "Current loss : 0.90\n",
            "Current loss : 1.00\n",
            "Current loss : 0.83\n",
            "Current loss : 1.14\n",
            "Current loss : 0.96\n",
            "Current loss : 0.91\n",
            "Current loss : 0.87\n",
            "Current loss : 0.94\n",
            "Current loss : 1.00\n",
            "Current loss : 1.02\n",
            "Current loss : 0.93\n",
            "Current loss : 0.93\n",
            "Current loss : 1.09\n",
            "Current loss : 1.00\n",
            "Current loss : 0.94\n",
            "Current loss : 1.11\n",
            "Current loss : 1.09\n",
            "Current loss : 0.79\n",
            "Current loss : 1.00\n",
            "Current loss : 1.04\n",
            "Current loss : 0.88\n",
            "Current loss : 1.09\n",
            "Current loss : 0.94\n",
            "Current loss : 1.01\n",
            "Current loss : 0.87\n",
            "Current loss : 0.88\n",
            "Current loss : 0.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWuuLWprTTom",
        "colab_type": "code",
        "outputId": "dff58eba-0d84-4288-c467-55e3b45ff1ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-7.9879e+00, -1.0588e+00, -4.2652e-01],\n",
              "        [-4.8600e+01,  0.0000e+00, -6.0397e+01],\n",
              "        [-6.3864e+01,  0.0000e+00, -6.3916e+01],\n",
              "        [ 0.0000e+00, -3.1545e+02, -1.9885e+02],\n",
              "        [ 0.0000e+00, -2.8043e+02, -5.0497e+01],\n",
              "        [ 0.0000e+00, -3.9889e+01, -1.0874e+02],\n",
              "        [ 0.0000e+00, -2.2874e+01, -2.9230e+01],\n",
              "        [ 0.0000e+00, -1.2878e+02, -1.5468e+02],\n",
              "        [ 0.0000e+00, -5.8744e+02, -3.8296e+02],\n",
              "        [ 0.0000e+00, -2.8867e+02, -5.0404e+01],\n",
              "        [-9.3705e+01, -2.7942e+02,  0.0000e+00],\n",
              "        [-1.7382e+01, -9.1674e+01,  0.0000e+00],\n",
              "        [-2.3556e+01,  0.0000e+00, -7.1566e+01],\n",
              "        [-6.1738e+01, -5.3609e+01,  0.0000e+00],\n",
              "        [ 0.0000e+00, -3.7475e+02, -5.5885e+01],\n",
              "        [-6.8465e+02, -3.3675e+02,  0.0000e+00],\n",
              "        [-3.8895e+01, -7.6294e-06, -1.2345e+01],\n",
              "        [-1.4709e+02,  0.0000e+00, -3.1391e+01],\n",
              "        [-8.2675e+02,  0.0000e+00, -5.3040e+02],\n",
              "        [-1.3842e+02, -6.9520e+01,  0.0000e+00],\n",
              "        [-2.7895e+01, -3.3751e+01,  0.0000e+00],\n",
              "        [-1.7987e+02, -2.7351e+01,  0.0000e+00],\n",
              "        [-2.6606e+02,  0.0000e+00, -2.5752e+02],\n",
              "        [-2.8835e+02,  0.0000e+00, -1.8717e+02],\n",
              "        [-3.0316e+02, -3.4315e+02,  0.0000e+00],\n",
              "        [-1.0399e+02,  0.0000e+00, -9.7506e+01],\n",
              "        [-2.1513e+02,  0.0000e+00, -1.0972e+02],\n",
              "        [-2.7781e+02,  0.0000e+00, -1.8387e+02],\n",
              "        [-6.9170e+02,  0.0000e+00, -3.3319e+02],\n",
              "        [-1.0008e+02,  0.0000e+00, -3.8128e+01],\n",
              "        [ 0.0000e+00, -5.3140e+01, -3.0302e+02],\n",
              "        [-2.0417e+02,  0.0000e+00, -1.4964e+02]], device='cuda:0',\n",
              "       grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLhdNUeVXn_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = h5py.File(\"./drive/My Drive/data/X_test_eFVIB85.h5\", \"r\")[\"features\"][:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK5Wbzp8W6tR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = X[:, 11:]\n",
        "x_test_meta = X[:, :11]\n",
        "\n",
        "x_test  = x_test[:, ::2]\n",
        "x_test  = np.array([[j for i, j in enumerate(eeg[10:]) if i % 6] for eeg in x_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4H7icsjXTMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test  = np.clip(x_test, -205, 205)\n",
        "x_test = x_test / 205"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MneX_eIHXaGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_meta  = (x_test_meta - maxi) / (mini - maxi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7HhTErRXjLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test  = torch.tensor(x_test).float().cuda()\n",
        "x_test_meta = torch.tensor(x_test_meta).float().cuda()\n",
        "x_test  = x_test.view((-1, 1, 512))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDpjP4kTX-kP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data  = utils.TensorDataset(x_test, x_test_meta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7OXDTBCYDav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loader  = utils.DataLoader(test_data, batch_size=32, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQETp_P3YH2B",
        "colab_type": "code",
        "outputId": "408d33b0-6c4e-4484-f2d5-585f7b917582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "net.eval()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc1): Linear(in_features=139, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
              "  (fc3): Linear(in_features=32, out_features=3, bias=True)\n",
              "  (drop1): Dropout(p=0.3, inplace=False)\n",
              "  (drop2): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUixLX_bYNLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out = []\n",
        "for x, meta in test_loader:\n",
        "  embedding = model(x)\n",
        "  output = net(meta, embedding)\n",
        "  out.extend(output.cpu().detach())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuxXRarVYwZp",
        "colab_type": "code",
        "outputId": "a947629f-1d86-4e2a-fcba-3439d0c68c8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        }
      },
      "source": [
        "out = np.array(out)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-31a10cbe1592>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9piqJ6idYw17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out = np.array([i.cpu().detach().numpy() for i in out])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KpUZfHAZ8Db",
        "colab_type": "code",
        "outputId": "2669bc70-97c9-4572-df8c-a96738160cb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "out.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(238366, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6SEZM7waLVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = np.argmax(out, axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o18spycRaR8N",
        "colab_type": "code",
        "outputId": "e2dcdb6e-1a5c-46f2-950c-a936cd028936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "238366"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SokqB6g5aTHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otknhg8_aYtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['id'] = range(238366)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmOQLj7PafpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['label'] = y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtsrfKi3aiic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('./drive/My Drive/data/cc.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOGBoqvqalRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}