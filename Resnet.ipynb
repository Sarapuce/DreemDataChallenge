{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SInXhoCEPaRe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "52af04ce-91b3-4290-ce59-fc7dc2d7cc00"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiVznudbSenQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, cv2, h5py\n",
        "import torch\n",
        "import torchvision\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as PathEffects\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as utils\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from copy import deepcopy\n",
        "from time import time\n",
        "from mpl_toolkits import mplot3d\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "from torch import autograd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9BSSWEsUOa1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "221ae3ba-22c6-41b2-f326-92a9090ffd01"
      },
      "source": [
        "X = h5py.File(\"./drive/My Drive/data/X_train.h5\", \"r\")[\"features\"][:]\n",
        "Y = pd.read_csv(\"./drive/My Drive/data/y_train.csv\").as_matrix()[:, 1].squeeze()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wDVZUl0Vp4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = len(X)\n",
        "x_train = X[:int(0.7*n), 11:]\n",
        "y_train = Y[:int(0.7*n)]\n",
        "x_test  = X[int(0.7*n):, 11:]\n",
        "y_test  = Y[int(0.7*n):]\n",
        "x_train = x_train[:, ::2]\n",
        "x_test  = x_test[:, ::2]\n",
        "x_train = np.array([[j for i, j in enumerate(eeg[10:]) if i % 6] for eeg in x_train])\n",
        "x_test  = np.array([[j for i, j in enumerate(eeg[10:]) if i % 6] for eeg in x_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNgP_2WAWjqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.clip(x_train, -205, 205)\n",
        "x_test  = np.clip(x_test, -205, 205)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhQ9K8fEbm14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train / 205\n",
        "x_test  = x_test / 205"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E56FCcjGex10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "def conv5x5(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv1d(in_planes, out_planes, kernel_size=5, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "def conv7x7(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv1d(in_planes, out_planes, kernel_size=7, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "\n",
        "class BasicBlock3x3(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes3, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock3x3, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes3, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm1d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm1d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class BasicBlock5x5(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes5, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock5x5, self).__init__()\n",
        "        self.conv1 = conv5x5(inplanes5, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm1d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv5x5(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm1d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        d = residual.shape[2] - out.shape[2]\n",
        "        out1 = residual[:,:,0:-d] + out\n",
        "        out1 = self.relu(out1)\n",
        "        # out += residual\n",
        "\n",
        "        return out1\n",
        "\n",
        "\n",
        "\n",
        "class BasicBlock7x7(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes7, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock7x7, self).__init__()\n",
        "        self.conv1 = conv7x7(inplanes7, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm1d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv7x7(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm1d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        d = residual.shape[2] - out.shape[2]\n",
        "        out1 = residual[:, :, 0:-d] + out\n",
        "        out1 = self.relu(out1)\n",
        "        # out += residual\n",
        "\n",
        "        return out1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MSResNet(nn.Module):\n",
        "    def __init__(self, input_channel, layers=[1, 1, 1, 1], num_classes=10):\n",
        "        self.inplanes3 = 64\n",
        "        self.inplanes5 = 64\n",
        "        self.inplanes7 = 64\n",
        "\n",
        "        super(MSResNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(input_channel, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer3x3_1 = self._make_layer3(BasicBlock3x3, 64, layers[0], stride=2)\n",
        "        self.layer3x3_2 = self._make_layer3(BasicBlock3x3, 128, layers[1], stride=2)\n",
        "        self.layer3x3_3 = self._make_layer3(BasicBlock3x3, 256, layers[2], stride=2)\n",
        "        # self.layer3x3_4 = self._make_layer3(BasicBlock3x3, 512, layers[3], stride=2)\n",
        "\n",
        "        # maxplooing kernel size: 16, 11, 6\n",
        "        self.maxpool3 = nn.AvgPool1d(kernel_size=16, stride=1, padding=0)\n",
        "\n",
        "\n",
        "        self.layer5x5_1 = self._make_layer5(BasicBlock5x5, 64, layers[0], stride=2)\n",
        "        self.layer5x5_2 = self._make_layer5(BasicBlock5x5, 128, layers[1], stride=2)\n",
        "        self.layer5x5_3 = self._make_layer5(BasicBlock5x5, 256, layers[2], stride=2)\n",
        "        # self.layer5x5_4 = self._make_layer5(BasicBlock5x5, 512, layers[3], stride=2)\n",
        "        self.maxpool5 = nn.AvgPool1d(kernel_size=11, stride=1, padding=0)\n",
        "\n",
        "\n",
        "        self.layer7x7_1 = self._make_layer7(BasicBlock7x7, 64, layers[0], stride=2)\n",
        "        self.layer7x7_2 = self._make_layer7(BasicBlock7x7, 128, layers[1], stride=2)\n",
        "        self.layer7x7_3 = self._make_layer7(BasicBlock7x7, 256, layers[2], stride=2)\n",
        "        # self.layer7x7_4 = self._make_layer7(BasicBlock7x7, 512, layers[3], stride=2)\n",
        "        self.maxpool7 = nn.AvgPool1d(kernel_size=6, stride=1, padding=0)\n",
        "\n",
        "        # self.drop = nn.Dropout(p=0.2)\n",
        "        self.fc = nn.Linear(256*3, 128)\n",
        "\n",
        "        # todo: modify the initialization\n",
        "        # for m in self.modules():\n",
        "        #     if isinstance(m, nn.Conv1d):\n",
        "        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "        #     elif isinstance(m, nn.BatchNorm1d):\n",
        "        #         m.weight.data.fill_(1)\n",
        "        #         m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer3(self, block, planes, blocks, stride=2):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes3 != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.inplanes3, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes3, planes, stride, downsample))\n",
        "        self.inplanes3 = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes3, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_layer5(self, block, planes, blocks, stride=2):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes5 != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.inplanes5, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes5, planes, stride, downsample))\n",
        "        self.inplanes5 = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes5, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def _make_layer7(self, block, planes, blocks, stride=2):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes7 != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.inplanes7, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes7, planes, stride, downsample))\n",
        "        self.inplanes7 = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes7, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x0):\n",
        "        x0 = self.conv1(x0)\n",
        "        x0 = self.bn1(x0)\n",
        "        x0 = self.relu(x0)\n",
        "        x0 = self.maxpool(x0)\n",
        "        x = self.layer3x3_1(x0)\n",
        "        x = self.layer3x3_2(x)\n",
        "        x = self.layer3x3_3(x)\n",
        "        # x = self.layer3x3_4(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        y = self.layer5x5_1(x0)\n",
        "        y = self.layer5x5_2(y)\n",
        "        y = self.layer5x5_3(y)\n",
        "        # y = self.layer5x5_4(y)\n",
        "        y = self.maxpool5(y)\n",
        "\n",
        "        z = self.layer7x7_1(x0)\n",
        "        z = self.layer7x7_2(z)\n",
        "        z = self.layer7x7_3(z)\n",
        "        # z = self.layer7x7_4(z)\n",
        "        z = self.maxpool7(z)\n",
        "\n",
        "        out = torch.cat([x, y, z], dim=1)\n",
        "        \n",
        "        out = out.squeeze()\n",
        "        # out = self.drop(out)\n",
        "        out1 = self.fc(out)\n",
        "\n",
        "        return out1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Cg0L7hoqRl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_distance(data):\n",
        "    dot_product = torch.matmul(data, data.transpose(1, 0))\n",
        "    diag        = torch.diag(dot_product)\n",
        "    distances   = diag.view((1, diag.shape[0])) - 2*dot_product + diag.view((diag.shape[0], 1))\n",
        "    distances   = torch.max(distances, torch.tensor([0]).float().cuda())\n",
        "    mask        = torch.eq(distances, torch.zeros(1).cuda()).float().cuda()\n",
        "    distances   = distances + mask * 1e-16\n",
        "    distances   = torch.sqrt(distances)\n",
        "    distances   = distances * (1.0 - mask)\n",
        "    return distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zf0eaC9qSh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mask_triplet(labels):\n",
        "    hot = F.one_hot(labels, 3).float()\n",
        "    positive_mask = torch.matmul(hot, hot.transpose(0, 1))\n",
        "    size = positive_mask.shape[0]\n",
        "    positive_mask[range(size), range(size)] = torch.zeros(positive_mask.shape[0]).long().cuda()\n",
        "    negative_mask = torch.matmul(torch.ones(hot.shape).long().cuda() - hot, hot.transpose(0, 1))\n",
        "    mask = torch.matmul(positive_mask.view((size, size, 1)), negative_mask.view((size, 1, size)))\n",
        "    return mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ALGO3A0qX6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def triplet_loss(data, labels, m):\n",
        "    distances = get_distance(data)\n",
        "    size = distances.shape[0]\n",
        "    delta_p   = distances.view((size, size, 1)).repeat((1, 1, size)) + m\n",
        "    delta_n   = distances.view((size, 1, size)).repeat((1, size, 1))\n",
        "    triplet_loss = delta_p - delta_n\n",
        "    mask_hard = torch.gt(delta_p, delta_n).float()\n",
        "    mask      = get_mask_triplet(labels).float() * mask_hard\n",
        "    triplet_loss = triplet_loss * mask\n",
        "    triplet_loss = torch.max(triplet_loss, torch.zeros(1).float().cuda())\n",
        "    \n",
        "    nb_of_tensor = torch.gt(triplet_loss, torch.tensor([1e-16]).float().cuda())\n",
        "    nb_of_tensor = torch.sum(nb_of_tensor)\n",
        "    triplet_loss = torch.sum(triplet_loss)\n",
        "    return triplet_loss / (nb_of_tensor + torch.tensor([1e-16]).float().cuda())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfhbxWgVrFXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augment(serie):\n",
        "    noise = torch.randn(serie.shape[0], 1, serie.shape[-1]).float().cuda() * 0.005\n",
        "    serie = noise + serie\n",
        "    coins = torch.gt(torch.randn((serie.shape[0], 1, 1)).float().cuda(), torch.tensor([0]).float().cuda()).float().cuda() * 2 - 1\n",
        "    serie = serie * coins\n",
        "    return serie"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKMWZmi9ri-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = torch.tensor(x_train).float().cuda()\n",
        "x_test  = torch.tensor(x_test).float().cuda()\n",
        "y_train = torch.tensor(y_train).long().cuda()\n",
        "y_test  = torch.tensor(y_test).long().cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KewAMnFCroWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.view((-1, 1, 512))\n",
        "x_test  = x_test.view((-1, 1, 512))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh55GzGjrr31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = utils.TensorDataset(x_train, y_train)\n",
        "test_data  = utils.TensorDataset(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ5pAM93sAVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = utils.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader  = utils.DataLoader(test_data, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SWwoXlEbpQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MSResNet(1).cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.009)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbnpjwPtfVfF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe2a8443-fe68-43d3-d0a1-399cd83a5c36"
      },
      "source": [
        "epochs = 100\n",
        "best_loss = 1000\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    test_loss    = 0\n",
        "    for x, y in train_loader:\n",
        "        x = augment(x)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss   = triplet_loss(output, y, torch.tensor([1]).float().cuda())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        print('Current loss : {0:.2f}'.format(loss.item()))\n",
        "    else:\n",
        "      for x, y in test_loader:\n",
        "        out  = model(x)\n",
        "        loss = loss_function(out, y)\n",
        "        test_loss += loss.item()\n",
        "      test_loss /= len(test_loader)\n",
        "\n",
        "      if test_loss < best_loss:\n",
        "        torch.save(model, './drive/My Drive/data/model.pt')\n",
        "        best_loss  = test_loss\n",
        "      print('Test loss : {}, Best loss : {}'.format(test_loss, best_loss))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current loss : 1.05\n",
            "Current loss : 1.07\n",
            "Current loss : 1.08\n",
            "Current loss : 1.11\n",
            "Current loss : 1.14\n",
            "Current loss : 1.01\n",
            "Current loss : 0.94\n",
            "Current loss : 1.05\n",
            "Current loss : 1.06\n",
            "Current loss : 1.12\n",
            "Current loss : 1.12\n",
            "Current loss : 1.19\n",
            "Current loss : 1.04\n",
            "Current loss : 1.11\n",
            "Current loss : 0.94\n",
            "Current loss : 1.16\n",
            "Current loss : 1.08\n",
            "Current loss : 1.09\n",
            "Current loss : 0.94\n",
            "Current loss : 1.07\n",
            "Current loss : 1.15\n",
            "Current loss : 1.06\n",
            "Current loss : 1.15\n",
            "Current loss : 1.07\n",
            "Current loss : 1.04\n",
            "Current loss : 1.05\n",
            "Current loss : 1.13\n",
            "Current loss : 1.09\n",
            "Current loss : 1.00\n",
            "Current loss : 1.11\n",
            "Current loss : 1.17\n",
            "Current loss : 1.03\n",
            "Current loss : 1.08\n",
            "Current loss : 1.01\n",
            "Current loss : 1.13\n",
            "Current loss : 1.03\n",
            "Current loss : 1.06\n",
            "Current loss : 1.11\n",
            "Current loss : 1.06\n",
            "Current loss : 1.00\n",
            "Current loss : 1.04\n",
            "Current loss : 1.01\n",
            "Current loss : 0.97\n",
            "Current loss : 1.02\n",
            "Current loss : 1.00\n",
            "Current loss : 1.03\n",
            "Current loss : 1.05\n",
            "Current loss : 1.01\n",
            "Current loss : 1.07\n",
            "Current loss : 0.94\n",
            "Current loss : 1.07\n",
            "Current loss : 1.04\n",
            "Current loss : 1.03\n",
            "Current loss : 1.03\n",
            "Current loss : 1.04\n",
            "Current loss : 1.05\n",
            "Current loss : 1.03\n",
            "Current loss : 1.09\n",
            "Current loss : 0.96\n",
            "Current loss : 1.06\n",
            "Current loss : 1.03\n",
            "Current loss : 1.00\n",
            "Current loss : 1.04\n",
            "Current loss : 1.12\n",
            "Current loss : 1.10\n",
            "Current loss : 1.02\n",
            "Current loss : 1.01\n",
            "Current loss : 0.93\n",
            "Current loss : 0.97\n",
            "Current loss : 1.10\n",
            "Current loss : 1.07\n",
            "Current loss : 1.14\n",
            "Current loss : 1.16\n",
            "Current loss : 1.10\n",
            "Current loss : 1.02\n",
            "Current loss : 1.05\n",
            "Current loss : 1.00\n",
            "Current loss : 1.03\n",
            "Current loss : 1.00\n",
            "Current loss : 0.99\n",
            "Current loss : 1.03\n",
            "Current loss : 0.99\n",
            "Current loss : 0.97\n",
            "Current loss : 1.00\n",
            "Current loss : 1.14\n",
            "Current loss : 0.98\n",
            "Current loss : 1.15\n",
            "Current loss : 1.19\n",
            "Current loss : 1.20\n",
            "Current loss : 1.14\n",
            "Current loss : 1.05\n",
            "Current loss : 0.97\n",
            "Current loss : 0.95\n",
            "Current loss : 1.01\n",
            "Current loss : 1.00\n",
            "Current loss : 1.01\n",
            "Current loss : 1.03\n",
            "Current loss : 1.01\n",
            "Current loss : 1.04\n",
            "Current loss : 1.02\n",
            "Current loss : 1.07\n",
            "Current loss : 1.06\n",
            "Current loss : 1.04\n",
            "Current loss : 0.92\n",
            "Current loss : 1.00\n",
            "Current loss : 0.93\n",
            "Current loss : 0.92\n",
            "Current loss : 1.06\n",
            "Current loss : 1.15\n",
            "Current loss : 1.11\n",
            "Current loss : 1.06\n",
            "Current loss : 1.03\n",
            "Current loss : 0.97\n",
            "Current loss : 1.04\n",
            "Current loss : 0.99\n",
            "Current loss : 1.04\n",
            "Current loss : 0.99\n",
            "Current loss : 1.05\n",
            "Current loss : 1.21\n",
            "Current loss : 1.06\n",
            "Current loss : 0.95\n",
            "Current loss : 0.99\n",
            "Current loss : 1.01\n",
            "Current loss : 0.95\n",
            "Current loss : 0.90\n",
            "Current loss : 1.03\n",
            "Current loss : 1.16\n",
            "Current loss : 1.13\n",
            "Current loss : 1.14\n",
            "Current loss : 1.10\n",
            "Current loss : 1.15\n",
            "Current loss : 1.00\n",
            "Current loss : 1.08\n",
            "Current loss : 1.06\n",
            "Current loss : 1.01\n",
            "Current loss : 0.96\n",
            "Current loss : 1.00\n",
            "Current loss : 1.06\n",
            "Current loss : 1.09\n",
            "Current loss : 1.00\n",
            "Current loss : 1.01\n",
            "Current loss : 1.07\n",
            "Current loss : 1.28\n",
            "Current loss : 1.25\n",
            "Current loss : 1.14\n",
            "Current loss : 1.13\n",
            "Current loss : 1.00\n",
            "Current loss : 0.97\n",
            "Current loss : 0.98\n",
            "Current loss : 0.97\n",
            "Current loss : 0.98\n",
            "Current loss : 1.00\n",
            "Current loss : 0.97\n",
            "Current loss : 0.96\n",
            "Current loss : 0.97\n",
            "Current loss : 0.94\n",
            "Current loss : 1.00\n",
            "Current loss : 1.12\n",
            "Current loss : 1.07\n",
            "Current loss : 1.14\n",
            "Current loss : 1.17\n",
            "Current loss : 1.02\n",
            "Current loss : 1.05\n",
            "Current loss : 0.94\n",
            "Current loss : 0.96\n",
            "Current loss : 0.88\n",
            "Current loss : 1.07\n",
            "Current loss : 1.09\n",
            "Current loss : 0.98\n",
            "Current loss : 1.08\n",
            "Current loss : 1.04\n",
            "Current loss : 1.04\n",
            "Current loss : 0.98\n",
            "Current loss : 0.97\n",
            "Current loss : 1.07\n",
            "Current loss : 1.08\n",
            "Current loss : 0.98\n",
            "Current loss : 1.14\n",
            "Current loss : 1.01\n",
            "Current loss : 1.03\n",
            "Current loss : 1.00\n",
            "Current loss : 1.01\n",
            "Current loss : 0.98\n",
            "Current loss : 1.01\n",
            "Current loss : 0.95\n",
            "Current loss : 0.98\n",
            "Current loss : 1.07\n",
            "Current loss : 1.01\n",
            "Current loss : 1.00\n",
            "Current loss : 1.04\n",
            "Current loss : 1.13\n",
            "Current loss : 1.12\n",
            "Current loss : 1.12\n",
            "Current loss : 1.05\n",
            "Current loss : 1.06\n",
            "Current loss : 1.00\n",
            "Current loss : 0.92\n",
            "Current loss : 0.96\n",
            "Current loss : 0.93\n",
            "Current loss : 0.98\n",
            "Current loss : 0.96\n",
            "Current loss : 0.95\n",
            "Current loss : 1.07\n",
            "Current loss : 1.03\n",
            "Current loss : 1.00\n",
            "Current loss : 1.11\n",
            "Current loss : 1.13\n",
            "Current loss : 1.12\n",
            "Current loss : 1.01\n",
            "Current loss : 1.09\n",
            "Current loss : 1.01\n",
            "Current loss : 0.99\n",
            "Current loss : 0.98\n",
            "Current loss : 0.96\n",
            "Current loss : 0.96\n",
            "Current loss : 1.03\n",
            "Current loss : 0.98\n",
            "Current loss : 0.94\n",
            "Current loss : 1.05\n",
            "Current loss : 1.09\n",
            "Current loss : 0.95\n",
            "Current loss : 1.10\n",
            "Current loss : 1.02\n",
            "Current loss : 0.98\n",
            "Current loss : 0.99\n",
            "Current loss : 1.00\n",
            "Current loss : 1.04\n",
            "Current loss : 1.00\n",
            "Current loss : 0.99\n",
            "Current loss : 1.08\n",
            "Current loss : 0.98\n",
            "Current loss : 1.02\n",
            "Current loss : 1.03\n",
            "Current loss : 1.06\n",
            "Current loss : 1.05\n",
            "Current loss : 1.03\n",
            "Current loss : 1.05\n",
            "Current loss : 0.93\n",
            "Current loss : 1.04\n",
            "Current loss : 1.01\n",
            "Current loss : 1.04\n",
            "Current loss : 1.02\n",
            "Current loss : 1.01\n",
            "Current loss : 1.04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-ae546882e08c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtriplet_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Current loss : {0:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdt8-YWVfYRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = torch.from_numpy(test).view(1, 1, 512).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrbUP7QEfdgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cc = model(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buGtfVZulFYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}